{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suitable-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import emot\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS #These are Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "devoted-favor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\TensorFlowGPU\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Use this to extend the viewing width of Jupyter\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "quantitative-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mbti_1.csv')\n",
    "df[\"posts\"]= df['posts'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "australian-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing web address, ..., numbers, and delimiters(|||)\n",
    "def cleaner(comments):\n",
    "    #comments = re.sub('http\\S+ ','',comments)#removing web addresses\n",
    "    comments = re.sub('\\w+:\\/\\/\\S+','',comments)#removing web addresses\n",
    "    comments = re.sub('\\.\\.\\.','', comments)#removing triple...\n",
    "    comments = re.sub('[0-9]+','', comments)#removing numbers\n",
    "    comments = re.sub('\\|\\|\\|', '', comments)# removing delimiters\n",
    "    for emote, mean in EMOTICONS.items():\n",
    "        comments = comments.replace(emote, ' <'+mean+'> ')\n",
    "    comments = re.sub(r':(\\w+):', r' <\\1> ', comments) # we are capturing group1 :group1: and returning <group1>\n",
    "    return comments\n",
    "df['Clean']= df['posts'].apply(lambda x: cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "composed-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uncomment if you neeed to Clean_v1.csv \n",
    "\"\"\"\n",
    "#df.to_csv(\"Clean_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "australian-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually Replacing Emojis that are not compatible to the EMOTICONS \n",
    "def emoji(comments):\n",
    "    comments = re.sub('\\@\\.\\@', ' <confused> ',comments)\n",
    "    comments=re.sub('\\:D', ' <happy> ', comments)\n",
    "    comments=re.sub('\\>\\.\\>',' <opsies> ',comments)\n",
    "    comments=re.sub('\\<\\.\\<',' <opsies> ',comments)\n",
    "    comments=re.sub('\\;D',' <winky> ',comments)\n",
    "    comments=re.sub('\\:\\)',' <happy> ',comments)\n",
    "    comments= re.sub(\"\\>\\.\\<\", ' <opsies> ', comments)\n",
    "    comments= re.sub(\"\\>\\<\", ' <opsies> ', comments)\n",
    "    comments= re.sub(\"\\)\\:\", ' <sad> ', comments)\n",
    "    comments= re.sub(\"xD\", ' <laughing> ', comments)\n",
    "    comments= re.sub(\"\\:P\", ' <naughty> ', comments)\n",
    "    comments= re.sub(\"\\;\\)\", ' <winky> ', comments)\n",
    "    return comments\n",
    "df['Clean']=df['Clean'].apply(lambda x: emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "turned-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Stop Words NLTK\n",
    "from nltk.corpus import stopwords\n",
    "stopwordlist = stopwords.words('english')\n",
    "\n",
    "def stopWord(comments):\n",
    "    lst = []\n",
    "    wordList =nltk.word_tokenize(comments)\n",
    "    #wordList = comments.split()\n",
    "    for word in wordList:\n",
    "        if word.lower() not in stopwordlist:\n",
    "            lst.append(word.lower())\n",
    "    return lst\n",
    "df['StopWord']= df['Clean'].apply(lambda x: stopWord(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "defined-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize/Stemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "def lemmatize_words(posts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # lemmatization process depends on the POS tag to come up with the correct lemma, so we set up word net map\n",
    "    stem_words = []\n",
    "    wordnet_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "    tagged = nltk.pos_tag(posts)\n",
    "    stemmed = [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in tagged]\n",
    "    return stemmed\n",
    "df['Lemmatize']= df['StopWord'].apply(lambda x: lemmatize_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tired-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuation\n",
    "symbollist = '''[\\?\\\"\\!\\@\\#\\$\\%\\^\\&\\*\\'\\(\\)\\+\\~\\[\\]\\{\\}\\;\\:\\`\\~\\-\\/\\\\\\|\\,\\.\\_]''' \n",
    "def punc_remove(x):\n",
    "    b=[]\n",
    "    for i in x:\n",
    "        g=[]\n",
    "        for _ in i:\n",
    "            word = re.sub(symbollist,'',_)\n",
    "            if len(word)>0:\n",
    "                g.append(word)\n",
    "        b.append(g)\n",
    "    return b\n",
    "df['New']=punc_remove(df['Lemmatize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hired-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export DF to csv\n",
    "df.to_csv('clean_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-sphere",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
