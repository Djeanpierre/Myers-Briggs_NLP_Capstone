{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stuffed-spirituality",
   "metadata": {
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1619649980145,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "cardiac-library"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import gensim\n",
    "import ast\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "textile-kansas",
   "metadata": {
    "id": "r6jc3hGMt_3N"
   },
   "outputs": [],
   "source": [
    "#Most Common 50 ADJs (to skip this step, use adjs.pickle)\n",
    "cleanv2 = pd.read_csv('clean_v2.csv')\n",
    "    \n",
    "def feature_common_adjs(words):\n",
    "    words = eval(words)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    adjs = [x for x, y in tagged if y.startswith('JJ') and x.isalpha() and len(x)>2]\n",
    "    common_adjs = [(x,y) for x, y in nltk.FreqDist(adjs).most_common(5)]\n",
    "    return {'most_common_adjs' : common_adjs}\n",
    "\n",
    "cleanv2['ADJs'] = cleanv2['New'].apply(lambda post: feature_common_adjs(post))\n",
    "\n",
    "with open('adjs.pickle', 'wb') as handle:\n",
    "    pickle.dump(cleanv2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "economic-import",
   "metadata": {
    "id": "l2wJ4WB3uDiF"
   },
   "outputs": [],
   "source": [
    "#to skip this step, use adjs_v2.pickle\n",
    "cleanv2 = pd.read_pickle('adjs.pickle')\n",
    "\n",
    "wordlist = {}\n",
    "\n",
    "for i in cleanv2['ADJs']:\n",
    "    for zipped in i.values():\n",
    "        for word,count in zipped:\n",
    "            if word not in wordlist.keys():\n",
    "                wordlist[word] = count\n",
    "            else:\n",
    "                wordlist[word] += count\n",
    "\n",
    "wordlist = dict(sorted(wordlist.items(), key=lambda item: item[1], reverse = True))\n",
    "common_adjs = [i for i in wordlist.keys()][:50]\n",
    "\n",
    "def adj_extractor(post):\n",
    "    feature_adj = {}\n",
    "    lst = eval(post)\n",
    "    lst = list(set(lst))\n",
    "    for i in common_adjs:\n",
    "        if i in lst:\n",
    "            feature_adj[i] = 1\n",
    "        else:\n",
    "            feature_adj[i] = 0\n",
    "    return(feature_adj)\n",
    "\n",
    "cleanv2['common_adjs'] = cleanv2['New'].apply(lambda x: adj_extractor(x))\n",
    "\n",
    "with open('adjs_v2.pickle', 'wb') as handle:\n",
    "    pickle.dump(cleanv2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "adverse-intensity",
   "metadata": {
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1619649998903,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "intensive-wedding"
   },
   "outputs": [],
   "source": [
    "adjDF = pd.read_pickle('adjs_v2.pickle')\n",
    "featureDF = pd.read_pickle('PickleFeatureSet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "requested-tyler",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1619650002898,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "subtle-adjustment",
    "outputId": "9a71f235-ee12-4d7f-fc35-6046fa4f83ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>EmojiFeature</th>\n",
       "      <th>ADJs</th>\n",
       "      <th>AvgSentLength</th>\n",
       "      <th>MostCommonWord</th>\n",
       "      <th>AvgChar</th>\n",
       "      <th>Avg TFIDF</th>\n",
       "      <th>vaderScore</th>\n",
       "      <th>CombineDict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>{'&lt; happy &gt;': False, '&lt; laugh &gt;': False, '&lt; fr...</td>\n",
       "      <td>59</td>\n",
       "      <td>13.6</td>\n",
       "      <td>92</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.042</td>\n",
       "      <td>2</td>\n",
       "      <td>({'&lt; happy &gt;': False, '&lt; laugh &gt;': False, '&lt; f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>{'&lt; happy &gt;': True, '&lt; laugh &gt;': False, '&lt; fro...</td>\n",
       "      <td>104</td>\n",
       "      <td>11.8</td>\n",
       "      <td>112</td>\n",
       "      <td>5.52</td>\n",
       "      <td>0.023</td>\n",
       "      <td>2</td>\n",
       "      <td>({'&lt; happy &gt;': True, '&lt; laugh &gt;': False, '&lt; fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>{'&lt; happy &gt;': True, '&lt; laugh &gt;': False, '&lt; fro...</td>\n",
       "      <td>67</td>\n",
       "      <td>14.3</td>\n",
       "      <td>92</td>\n",
       "      <td>5.84</td>\n",
       "      <td>0.032</td>\n",
       "      <td>2</td>\n",
       "      <td>({'&lt; happy &gt;': True, '&lt; laugh &gt;': False, '&lt; fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>{'&lt; happy &gt;': False, '&lt; laugh &gt;': False, '&lt; fr...</td>\n",
       "      <td>99</td>\n",
       "      <td>11.9</td>\n",
       "      <td>119</td>\n",
       "      <td>5.93</td>\n",
       "      <td>0.029</td>\n",
       "      <td>2</td>\n",
       "      <td>({'&lt; happy &gt;': False, '&lt; laugh &gt;': False, '&lt; f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>{'&lt; happy &gt;': False, '&lt; laugh &gt;': True, '&lt; fro...</td>\n",
       "      <td>67</td>\n",
       "      <td>11.5</td>\n",
       "      <td>97</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.032</td>\n",
       "      <td>2</td>\n",
       "      <td>({'&lt; happy &gt;': False, '&lt; laugh &gt;': True, '&lt; fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                       EmojiFeature  ADJs  \\\n",
       "0  INFJ  {'< happy >': False, '< laugh >': False, '< fr...    59   \n",
       "1  ENTP  {'< happy >': True, '< laugh >': False, '< fro...   104   \n",
       "2  INTP  {'< happy >': True, '< laugh >': False, '< fro...    67   \n",
       "3  INTJ  {'< happy >': False, '< laugh >': False, '< fr...    99   \n",
       "4  ENTJ  {'< happy >': False, '< laugh >': True, '< fro...    67   \n",
       "\n",
       "   AvgSentLength  MostCommonWord  AvgChar  Avg TFIDF  vaderScore  \\\n",
       "0           13.6              92     5.75      0.042           2   \n",
       "1           11.8             112     5.52      0.023           2   \n",
       "2           14.3              92     5.84      0.032           2   \n",
       "3           11.9             119     5.93      0.029           2   \n",
       "4           11.5              97     5.88      0.032           2   \n",
       "\n",
       "                                         CombineDict  \n",
       "0  ({'< happy >': False, '< laugh >': False, '< f...  \n",
       "1  ({'< happy >': True, '< laugh >': False, '< fr...  \n",
       "2  ({'< happy >': True, '< laugh >': False, '< fr...  \n",
       "3  ({'< happy >': False, '< laugh >': False, '< f...  \n",
       "4  ({'< happy >': False, '< laugh >': True, '< fr...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pulling values from dict of certain columns\n",
    "featureDF['ADJs'] = [j for i in featureDF['ADJs'] for j in i.values()]\n",
    "featureDF['AvgSentLength'] = [j for i in featureDF['AvgSentLength'] for j in i.values()]\n",
    "featureDF['AvgChar'] = [j for i in featureDF['AvgChar'] for j in i.values()]\n",
    "\n",
    "# Most common word\n",
    "most_com = []\n",
    "for i in featureDF['MostCommonWord']:\n",
    "    if len(i) == 0:\n",
    "        most_com.append(0)\n",
    "    else:\n",
    "        for j in i.values():\n",
    "            most_com.append(j)\n",
    "featureDF['MostCommonWord'] = most_com\n",
    "\n",
    "# Average TIIDF\n",
    "featureDF['Avg TFIDF'] = [j for i in featureDF['Avg TFIDF'] for j in i.values()]\n",
    "avg_tfidf = []\n",
    "for i in featureDF['Avg TFIDF']:\n",
    "    if i < 1:\n",
    "        avg_tfidf.append(i)\n",
    "    else:\n",
    "        avg_tfidf.append(0)\n",
    "\n",
    "featureDF['Avg TFIDF'] = avg_tfidf\n",
    "\n",
    "# vaderScore: convert the attribute to be a number, negative = 1/neutral = 2/positive = 3\n",
    "vader_list = []\n",
    "for i in featureDF['vaderScore']:\n",
    "    lst = list(i.values())\n",
    "    attribute = lst.index(max(lst[:2]))\n",
    "    if attribute == 0:\n",
    "        vader_list.append(1)  # negative\n",
    "    elif attribute == 1:\n",
    "        vader_list.append(2) # neutral\n",
    "    else:\n",
    "        vader_list.append(3) # positive\n",
    "featureDF['vaderScore'] = vader_list\n",
    "featureDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "federal-entity",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "executionInfo": {
     "elapsed": 2681,
     "status": "ok",
     "timestamp": 1619650019281,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "banner-weekend",
    "outputId": "6bbc189f-6386-4039-849c-9d8198cab36d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>Clean</th>\n",
       "      <th>New</th>\n",
       "      <th>ADJs</th>\n",
       "      <th>common_adjs</th>\n",
       "      <th>good</th>\n",
       "      <th>happy</th>\n",
       "      <th>much</th>\n",
       "      <th>cheeky</th>\n",
       "      <th>...</th>\n",
       "      <th>able</th>\n",
       "      <th>strong</th>\n",
       "      <th>human</th>\n",
       "      <th>enfp</th>\n",
       "      <th>interested</th>\n",
       "      <th>entp</th>\n",
       "      <th>emotional</th>\n",
       "      <th>easy</th>\n",
       "      <th>young</th>\n",
       "      <th>certain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>' and intj moments    sportscenter not top ten...</td>\n",
       "      <td>['intj', 'moment', 'sportscenter', 'top', 'ten...</td>\n",
       "      <td>{'most_common_adjs': [('good', 3), ('intj', 2)...</td>\n",
       "      <td>{'good': 1, 'happy': 0, 'much': 1, 'cheeky': 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>['m', 'find', 'lack', 'post', 'alarmingsex', '...</td>\n",
       "      <td>{'most_common_adjs': [('big', 11), ('happy', 5...</td>\n",
       "      <td>{'good': 1, 'happy': 1, 'much': 1, 'cheeky': 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>'Good one  _____    course, to which I say I k...</td>\n",
       "      <td>['good', 'one', '_____', 'course', 'say', 'kno...</td>\n",
       "      <td>{'most_common_adjs': [('good', 7), ('happy', 7...</td>\n",
       "      <td>{'good': 1, 'happy': 1, 'much': 1, 'cheeky': 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>['dear', 'intp', 'enjoyed', 'conversation', 'd...</td>\n",
       "      <td>{'most_common_adjs': [('social', 5), ('much', ...</td>\n",
       "      <td>{'good': 0, 'happy': 1, 'much': 1, 'cheeky': 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>'You're fired.That's another silly misconcepti...</td>\n",
       "      <td>['you', 're', 'firedthat', 's', 'another', 'si...</td>\n",
       "      <td>{'most_common_adjs': [('good', 6), ('best', 2)...</td>\n",
       "      <td>{'good': 1, 'happy': 0, 'much': 1, 'cheeky': 0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type                                              Clean  \\\n",
       "0           0     1  ' and intj moments    sportscenter not top ten...   \n",
       "1           1     0  'I'm finding the lack of me in these posts ver...   \n",
       "2           2     1  'Good one  _____    course, to which I say I k...   \n",
       "3           3     1  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4           4     0  'You're fired.That's another silly misconcepti...   \n",
       "\n",
       "                                                 New  \\\n",
       "0  ['intj', 'moment', 'sportscenter', 'top', 'ten...   \n",
       "1  ['m', 'find', 'lack', 'post', 'alarmingsex', '...   \n",
       "2  ['good', 'one', '_____', 'course', 'say', 'kno...   \n",
       "3  ['dear', 'intp', 'enjoyed', 'conversation', 'd...   \n",
       "4  ['you', 're', 'firedthat', 's', 'another', 'si...   \n",
       "\n",
       "                                                ADJs  \\\n",
       "0  {'most_common_adjs': [('good', 3), ('intj', 2)...   \n",
       "1  {'most_common_adjs': [('big', 11), ('happy', 5...   \n",
       "2  {'most_common_adjs': [('good', 7), ('happy', 7...   \n",
       "3  {'most_common_adjs': [('social', 5), ('much', ...   \n",
       "4  {'most_common_adjs': [('good', 6), ('best', 2)...   \n",
       "\n",
       "                                         common_adjs  good  happy  much  \\\n",
       "0  {'good': 1, 'happy': 0, 'much': 1, 'cheeky': 1...     1      0     1   \n",
       "1  {'good': 1, 'happy': 1, 'much': 1, 'cheeky': 1...     1      1     1   \n",
       "2  {'good': 1, 'happy': 1, 'much': 1, 'cheeky': 0...     1      1     1   \n",
       "3  {'good': 0, 'happy': 1, 'much': 1, 'cheeky': 0...     0      1     1   \n",
       "4  {'good': 1, 'happy': 0, 'much': 1, 'cheeky': 0...     1      0     1   \n",
       "\n",
       "   cheeky  ...  able  strong  human  enfp  interested  entp  emotional  easy  \\\n",
       "0       1  ...     0       0      0     1           0     1          0     0   \n",
       "1       1  ...     1       0      0     0           0     1          0     1   \n",
       "2       0  ...     1       0      1     0           1     0          0     0   \n",
       "3       0  ...     1       0      0     0           0     0          1     0   \n",
       "4       0  ...     0       0      0     0           1     0          0     1   \n",
       "\n",
       "   young  certain  \n",
       "0      0        0  \n",
       "1      0        0  \n",
       "2      0        0  \n",
       "3      0        1  \n",
       "4      0        0  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This we keep because Austin has already used it in his deep learning model\n",
    "list_a = list(enumerate(adjDF['common_adjs'][0].keys()))\n",
    "for index,lab in list_a:\n",
    "    adjDF[lab] = adjDF['common_adjs'].apply(lambda x: [v for k,v in x.items()][index])\n",
    "adjDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "hearing-allah",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1619650062020,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "unavailable-puppy",
    "outputId": "9cef6f1c-ded1-4b7a-94fd-493d9516722c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clean</th>\n",
       "      <th>good</th>\n",
       "      <th>happy</th>\n",
       "      <th>much</th>\n",
       "      <th>cheeky</th>\n",
       "      <th>playful</th>\n",
       "      <th>big</th>\n",
       "      <th>sure</th>\n",
       "      <th>many</th>\n",
       "      <th>little</th>\n",
       "      <th>...</th>\n",
       "      <th>able</th>\n",
       "      <th>strong</th>\n",
       "      <th>human</th>\n",
       "      <th>enfp</th>\n",
       "      <th>interested</th>\n",
       "      <th>entp</th>\n",
       "      <th>emotional</th>\n",
       "      <th>easy</th>\n",
       "      <th>young</th>\n",
       "      <th>certain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>' and intj moments    sportscenter not top ten...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Good one  _____    course, to which I say I k...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'You're fired.That's another silly misconcepti...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Clean  good  happy  much  \\\n",
       "0  ' and intj moments    sportscenter not top ten...     1      0     1   \n",
       "1  'I'm finding the lack of me in these posts ver...     1      1     1   \n",
       "2  'Good one  _____    course, to which I say I k...     1      1     1   \n",
       "3  'Dear INTP,   I enjoyed our conversation the o...     0      1     1   \n",
       "4  'You're fired.That's another silly misconcepti...     1      0     1   \n",
       "\n",
       "   cheeky  playful  big  sure  many  little  ...  able  strong  human  enfp  \\\n",
       "0       1        1    1     0     1       0  ...     0       0      0     1   \n",
       "1       1        0    1     0     1       1  ...     1       0      0     0   \n",
       "2       0        0    1     0     0       0  ...     1       0      1     0   \n",
       "3       0        1    0     1     1       0  ...     1       0      0     0   \n",
       "4       0        0    1     1     1       1  ...     0       0      0     0   \n",
       "\n",
       "   interested  entp  emotional  easy  young  certain  \n",
       "0           0     1          0     0      0        0  \n",
       "1           0     1          0     1      0        0  \n",
       "2           1     0          0     0      0        0  \n",
       "3           0     0          1     0      0        1  \n",
       "4           1     0          0     1      0        0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dropping all other columns except for oneHot and Clean\n",
    "oneHotADJ = adjDF.drop(['Unnamed: 0', 'type','New', 'ADJs', 'common_adjs'], axis = 1)\n",
    "oneHotADJ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "collect-protection",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1619650071283,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "ordinary-miracle",
    "outputId": "356cf140-b64d-4fdd-835e-4770209adf61"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>myerTypes</th>\n",
       "      <th>EmojiFeature</th>\n",
       "      <th>ADJs</th>\n",
       "      <th>AvgSentLength</th>\n",
       "      <th>MostCommonWord</th>\n",
       "      <th>AvgChar</th>\n",
       "      <th>Avg TFIDF</th>\n",
       "      <th>vaderScore</th>\n",
       "      <th>CombineDict</th>\n",
       "      <th>Clean</th>\n",
       "      <th>...</th>\n",
       "      <th>able</th>\n",
       "      <th>strong</th>\n",
       "      <th>human</th>\n",
       "      <th>enfp</th>\n",
       "      <th>interested</th>\n",
       "      <th>entp</th>\n",
       "      <th>emotional</th>\n",
       "      <th>easy</th>\n",
       "      <th>young</th>\n",
       "      <th>certain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>{'&lt; happy &gt;': False, '&lt; laugh &gt;': False, '&lt; fr...</td>\n",
       "      <td>59</td>\n",
       "      <td>13.6</td>\n",
       "      <td>92</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.042</td>\n",
       "      <td>2</td>\n",
       "      <td>({'&lt; happy &gt;': False, '&lt; laugh &gt;': False, '&lt; f...</td>\n",
       "      <td>' and intj moments    sportscenter not top ten...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>{'&lt; happy &gt;': True, '&lt; laugh &gt;': False, '&lt; fro...</td>\n",
       "      <td>104</td>\n",
       "      <td>11.8</td>\n",
       "      <td>112</td>\n",
       "      <td>5.52</td>\n",
       "      <td>0.023</td>\n",
       "      <td>2</td>\n",
       "      <td>({'&lt; happy &gt;': True, '&lt; laugh &gt;': False, '&lt; fr...</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>{'&lt; happy &gt;': True, '&lt; laugh &gt;': False, '&lt; fro...</td>\n",
       "      <td>67</td>\n",
       "      <td>14.3</td>\n",
       "      <td>92</td>\n",
       "      <td>5.84</td>\n",
       "      <td>0.032</td>\n",
       "      <td>2</td>\n",
       "      <td>({'&lt; happy &gt;': True, '&lt; laugh &gt;': False, '&lt; fr...</td>\n",
       "      <td>'Good one  _____    course, to which I say I k...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>{'&lt; happy &gt;': False, '&lt; laugh &gt;': False, '&lt; fr...</td>\n",
       "      <td>99</td>\n",
       "      <td>11.9</td>\n",
       "      <td>119</td>\n",
       "      <td>5.93</td>\n",
       "      <td>0.029</td>\n",
       "      <td>2</td>\n",
       "      <td>({'&lt; happy &gt;': False, '&lt; laugh &gt;': False, '&lt; f...</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>{'&lt; happy &gt;': False, '&lt; laugh &gt;': True, '&lt; fro...</td>\n",
       "      <td>67</td>\n",
       "      <td>11.5</td>\n",
       "      <td>97</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.032</td>\n",
       "      <td>2</td>\n",
       "      <td>({'&lt; happy &gt;': False, '&lt; laugh &gt;': True, '&lt; fr...</td>\n",
       "      <td>'You're fired.That's another silly misconcepti...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  myerTypes                                       EmojiFeature  ADJs  \\\n",
       "0      INFJ  {'< happy >': False, '< laugh >': False, '< fr...    59   \n",
       "1      ENTP  {'< happy >': True, '< laugh >': False, '< fro...   104   \n",
       "2      INTP  {'< happy >': True, '< laugh >': False, '< fro...    67   \n",
       "3      INTJ  {'< happy >': False, '< laugh >': False, '< fr...    99   \n",
       "4      ENTJ  {'< happy >': False, '< laugh >': True, '< fro...    67   \n",
       "\n",
       "   AvgSentLength  MostCommonWord  AvgChar  Avg TFIDF  vaderScore  \\\n",
       "0           13.6              92     5.75      0.042           2   \n",
       "1           11.8             112     5.52      0.023           2   \n",
       "2           14.3              92     5.84      0.032           2   \n",
       "3           11.9             119     5.93      0.029           2   \n",
       "4           11.5              97     5.88      0.032           2   \n",
       "\n",
       "                                         CombineDict  \\\n",
       "0  ({'< happy >': False, '< laugh >': False, '< f...   \n",
       "1  ({'< happy >': True, '< laugh >': False, '< fr...   \n",
       "2  ({'< happy >': True, '< laugh >': False, '< fr...   \n",
       "3  ({'< happy >': False, '< laugh >': False, '< f...   \n",
       "4  ({'< happy >': False, '< laugh >': True, '< fr...   \n",
       "\n",
       "                                               Clean  ...  able  strong  \\\n",
       "0  ' and intj moments    sportscenter not top ten...  ...     0       0   \n",
       "1  'I'm finding the lack of me in these posts ver...  ...     1       0   \n",
       "2  'Good one  _____    course, to which I say I k...  ...     1       0   \n",
       "3  'Dear INTP,   I enjoyed our conversation the o...  ...     1       0   \n",
       "4  'You're fired.That's another silly misconcepti...  ...     0       0   \n",
       "\n",
       "   human  enfp  interested  entp  emotional  easy  young  certain  \n",
       "0      0     1           0     1          0     0      0        0  \n",
       "1      0     0           0     1          0     1      0        0  \n",
       "2      1     0           1     0          0     0      0        0  \n",
       "3      0     0           0     0          1     0      0        1  \n",
       "4      0     0           1     0          0     1      0        0  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combinding the Two DF together\n",
    "df=pd.concat([featureDF,oneHotADJ], axis =1, join = 'inner')\n",
    "#Rename type column so when we turn the word list into a column it doesn't replace it.\n",
    "df = df.rename({'type': 'myerTypes'}, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "rotary-jaguar",
   "metadata": {
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1619650151849,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "focal-snapshot"
   },
   "outputs": [],
   "source": [
    "#Since Most Common Word(MCW) is same like ADJs we need to find out the particular word for that type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "growing-blink",
   "metadata": {
    "executionInfo": {
     "elapsed": 91478,
     "status": "ok",
     "timestamp": 1619650244491,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "reflected-kuwait"
   },
   "outputs": [],
   "source": [
    "#Cleaner function that filters out stopwords, punctuations, and keeps english characters\n",
    "stopwordlist = gensim.parsing.preprocessing.STOPWORDS\n",
    "def cleaner(post):\n",
    "    # remove words in < > or special symbols residuals\n",
    "    special_symbols_remove = re.sub('\\<[A-Za-z]+.[\\s\\w]*\\>|\\(|\\)|\\*|[A-Z]\\/[A-Z]|\\_+', '', post)\n",
    "    wordlist = nltk.word_tokenize(special_symbols_remove)\n",
    "    words_nonstop = [word.lower() for word in wordlist if word.lower() not in stopwordlist or word not in string.punctuation]\n",
    "    words = [word for word in words_nonstop if len(word) > 3] # words that length than 3\n",
    "    return words\n",
    "#We need CLean this so we have a list of words for each post to compare later on\n",
    "df['Cleaner'] = df['Clean'].apply(lambda post: cleaner(post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "rotary-construction",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 97528,
     "status": "ok",
     "timestamp": 1619650348586,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "broken-alpha",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "aafc7c20-eb53-42c4-c3ff-d736e07826e7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that',\n",
       " 'have',\n",
       " 'with',\n",
       " 'this',\n",
       " 'like',\n",
       " 'just',\n",
       " 'what',\n",
       " 'about',\n",
       " 'think',\n",
       " 'people',\n",
       " 'your',\n",
       " 'when',\n",
       " 'they',\n",
       " 'would',\n",
       " 'know',\n",
       " 'really',\n",
       " 'more',\n",
       " 'because',\n",
       " 'from',\n",
       " 'there',\n",
       " 'some',\n",
       " 'time',\n",
       " 'very',\n",
       " 'them',\n",
       " 'feel',\n",
       " 'much',\n",
       " 'being',\n",
       " 'been',\n",
       " 'love',\n",
       " 'other',\n",
       " 'things',\n",
       " 'good',\n",
       " 'want',\n",
       " 'something',\n",
       " 'will',\n",
       " 'most',\n",
       " 'only',\n",
       " 'also',\n",
       " 'then',\n",
       " 'could',\n",
       " 'than',\n",
       " 'well',\n",
       " 'does',\n",
       " 'always',\n",
       " 'even',\n",
       " 'type',\n",
       " 'here',\n",
       " 'someone',\n",
       " 'make',\n",
       " 'their',\n",
       " 'myself',\n",
       " 'never',\n",
       " 'life',\n",
       " 'find',\n",
       " 'into',\n",
       " 'thing',\n",
       " 'though',\n",
       " 'which',\n",
       " 'going',\n",
       " 'actually',\n",
       " 'person',\n",
       " 'sure',\n",
       " 'infp',\n",
       " 'right',\n",
       " 'pretty',\n",
       " 'were',\n",
       " 'first',\n",
       " 'friends',\n",
       " 'need',\n",
       " 'tongue',\n",
       " 'playful',\n",
       " 'sticking',\n",
       " 'infj',\n",
       " 'blowing',\n",
       " 'cheeky',\n",
       " 'same',\n",
       " 'raspberry',\n",
       " 'still',\n",
       " 'intj',\n",
       " 'work',\n",
       " 'thought',\n",
       " 'said',\n",
       " 'where',\n",
       " 'many',\n",
       " 'should',\n",
       " 'over',\n",
       " 'friend',\n",
       " 'take',\n",
       " 'read',\n",
       " 'around',\n",
       " 'probably',\n",
       " 'intp',\n",
       " 'best',\n",
       " 'thread',\n",
       " 'sometimes',\n",
       " 'years',\n",
       " 'mean',\n",
       " 'back',\n",
       " 'those',\n",
       " 'anything',\n",
       " 'maybe',\n",
       " 'little',\n",
       " 'kind',\n",
       " 'usually',\n",
       " 'while',\n",
       " 'post',\n",
       " 'after',\n",
       " 'better',\n",
       " 'look',\n",
       " 'might',\n",
       " 'long',\n",
       " 'different',\n",
       " 'enfp',\n",
       " 'these',\n",
       " 'before',\n",
       " 'makes',\n",
       " 'seems',\n",
       " 'ever',\n",
       " 'understand',\n",
       " 'others',\n",
       " 'world',\n",
       " 'everyone',\n",
       " 'since',\n",
       " 'trying',\n",
       " 'thinking',\n",
       " 'agree',\n",
       " 'quite',\n",
       " 'through',\n",
       " 'hard',\n",
       " 'both',\n",
       " 'point',\n",
       " 'made',\n",
       " 'come',\n",
       " 'tell',\n",
       " 'guess',\n",
       " 'doing',\n",
       " 'mind',\n",
       " 'types',\n",
       " 'believe',\n",
       " 'great',\n",
       " 'talk',\n",
       " 'school',\n",
       " 'every',\n",
       " 'feeling',\n",
       " 'down',\n",
       " 'entp',\n",
       " 'used',\n",
       " 'seem',\n",
       " 'give',\n",
       " 'everything',\n",
       " 'having',\n",
       " 'often',\n",
       " 'definitely',\n",
       " 'relationship',\n",
       " 'help',\n",
       " 'guys',\n",
       " 'last',\n",
       " 'personality',\n",
       " 'times',\n",
       " 'interesting',\n",
       " 'anyone',\n",
       " 'question',\n",
       " 'another',\n",
       " 'enough',\n",
       " 'part',\n",
       " 'idea',\n",
       " 'talking',\n",
       " 'else',\n",
       " 'hate',\n",
       " 'such',\n",
       " 'tend',\n",
       " 'getting',\n",
       " 'least',\n",
       " 'sense',\n",
       " 'keep',\n",
       " 'mbti',\n",
       " 'once',\n",
       " 'problem',\n",
       " 'care',\n",
       " 'true',\n",
       " 'either',\n",
       " 'without',\n",
       " 'year',\n",
       " 'found',\n",
       " 'stuff',\n",
       " 'yourself',\n",
       " 'nice',\n",
       " 'thanks',\n",
       " 'nothing',\n",
       " 'especially',\n",
       " 'music',\n",
       " 'between',\n",
       " 'reading',\n",
       " 'thank',\n",
       " 'experience',\n",
       " 'reason',\n",
       " 'wrong',\n",
       " 'however',\n",
       " 'high',\n",
       " 'again',\n",
       " 'sounds',\n",
       " 'start',\n",
       " 'yeah',\n",
       " 'rather',\n",
       " 'test',\n",
       " 'live',\n",
       " 'saying',\n",
       " 'looking',\n",
       " 'real',\n",
       " 'enjoy',\n",
       " 'feelings',\n",
       " 'almost',\n",
       " 'each',\n",
       " 'istj',\n",
       " 'social',\n",
       " 'done',\n",
       " 'away',\n",
       " 'forum',\n",
       " 'remember',\n",
       " 'happy',\n",
       " 'comes',\n",
       " 'close',\n",
       " 'sorry',\n",
       " 'fact',\n",
       " 'already',\n",
       " 'relate',\n",
       " 'exactly',\n",
       " 'wanted',\n",
       " 'answer',\n",
       " 'able',\n",
       " 'istp',\n",
       " 'girl',\n",
       " 'making',\n",
       " 'enfj',\n",
       " 'told',\n",
       " 'whole',\n",
       " 'place',\n",
       " 'until',\n",
       " 'functions',\n",
       " 'change',\n",
       " 'hope',\n",
       " 'entj',\n",
       " 'less',\n",
       " 'family',\n",
       " 'head',\n",
       " 'started',\n",
       " 'interested',\n",
       " 'alone',\n",
       " 'completely',\n",
       " 'isfp',\n",
       " 'using',\n",
       " 'seen',\n",
       " 'show',\n",
       " 'isfj',\n",
       " 'although',\n",
       " 'words',\n",
       " 'wish',\n",
       " 'past',\n",
       " 'welcome',\n",
       " 'certain',\n",
       " 'situation',\n",
       " 'sort',\n",
       " 'play',\n",
       " 'felt',\n",
       " 'days',\n",
       " 'along',\n",
       " 'general',\n",
       " 'infps',\n",
       " 'today',\n",
       " 'must',\n",
       " 'thoughts',\n",
       " 'took',\n",
       " 'emotional',\n",
       " 'course',\n",
       " 'become',\n",
       " 'mostly',\n",
       " 'example',\n",
       " 'stop',\n",
       " 'went',\n",
       " 'similar',\n",
       " 'ones',\n",
       " 'likely',\n",
       " 'emotions',\n",
       " 'sound',\n",
       " 'book',\n",
       " 'funny',\n",
       " 'totally',\n",
       " 'means',\n",
       " 'hear',\n",
       " 'based',\n",
       " 'frown',\n",
       " 'pouting',\n",
       " 'andry',\n",
       " 'favorite',\n",
       " 'personal',\n",
       " 'weird',\n",
       " 'important',\n",
       " 'dont',\n",
       " 'matter',\n",
       " 'strong']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a word list of all the words from the corpus\n",
    "all_word = []\n",
    "for i in range(len(df['Clean'])):\n",
    "    word = cleaner(df['Clean'][i])\n",
    "    for j in word:\n",
    "        all_word.append(j)\n",
    "\n",
    "fd = nltk.FreqDist(all_word)      \n",
    "most_common = fd.most_common(300)\n",
    "most = [z[0] for z in most_common]\n",
    "most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "complex-indonesia",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1619650380217,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "announced-toner",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('that', 'DT'),\n",
       " ('have', 'VBP'),\n",
       " ('with', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('like', 'JJ'),\n",
       " ('just', 'RB'),\n",
       " ('what', 'WP'),\n",
       " ('about', 'IN'),\n",
       " ('think', 'VBP'),\n",
       " ('people', 'NNS'),\n",
       " ('your', 'PRP$'),\n",
       " ('when', 'WRB'),\n",
       " ('they', 'PRP'),\n",
       " ('would', 'MD'),\n",
       " ('know', 'VB'),\n",
       " ('really', 'RB'),\n",
       " ('more', 'RBR'),\n",
       " ('because', 'IN'),\n",
       " ('from', 'IN'),\n",
       " ('there', 'EX'),\n",
       " ('some', 'DT'),\n",
       " ('time', 'NN'),\n",
       " ('very', 'RB'),\n",
       " ('them', 'PRP'),\n",
       " ('feel', 'VB'),\n",
       " ('much', 'JJ'),\n",
       " ('being', 'VBG'),\n",
       " ('been', 'VBN'),\n",
       " ('love', 'IN'),\n",
       " ('other', 'JJ'),\n",
       " ('things', 'NNS'),\n",
       " ('good', 'JJ'),\n",
       " ('want', 'VBP'),\n",
       " ('something', 'NN'),\n",
       " ('will', 'MD'),\n",
       " ('most', 'RBS'),\n",
       " ('only', 'RB'),\n",
       " ('also', 'RB'),\n",
       " ('then', 'RB'),\n",
       " ('could', 'MD'),\n",
       " ('than', 'IN'),\n",
       " ('well', 'RB'),\n",
       " ('does', 'VBZ'),\n",
       " ('always', 'RB'),\n",
       " ('even', 'RB'),\n",
       " ('type', 'NN'),\n",
       " ('here', 'RB'),\n",
       " ('someone', 'NN'),\n",
       " ('make', 'VBP'),\n",
       " ('their', 'PRP$'),\n",
       " ('myself', 'PRP'),\n",
       " ('never', 'RB'),\n",
       " ('life', 'NN'),\n",
       " ('find', 'VBP'),\n",
       " ('into', 'IN'),\n",
       " ('thing', 'NN'),\n",
       " ('though', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('going', 'VBG'),\n",
       " ('actually', 'RB'),\n",
       " ('person', 'NN'),\n",
       " ('sure', 'JJ'),\n",
       " ('infp', 'NN'),\n",
       " ('right', 'NN'),\n",
       " ('pretty', 'NN'),\n",
       " ('were', 'VBD'),\n",
       " ('first', 'JJ'),\n",
       " ('friends', 'NNS'),\n",
       " ('need', 'VBP'),\n",
       " ('tongue', 'JJ'),\n",
       " ('playful', 'JJ'),\n",
       " ('sticking', 'VBG'),\n",
       " ('infj', 'NN'),\n",
       " ('blowing', 'VBG'),\n",
       " ('cheeky', 'JJ'),\n",
       " ('same', 'JJ'),\n",
       " ('raspberry', 'NN'),\n",
       " ('still', 'RB'),\n",
       " ('intj', 'JJ'),\n",
       " ('work', 'NN'),\n",
       " ('thought', 'NN'),\n",
       " ('said', 'VBD'),\n",
       " ('where', 'WRB'),\n",
       " ('many', 'JJ'),\n",
       " ('should', 'MD'),\n",
       " ('over', 'VB'),\n",
       " ('friend', 'JJ'),\n",
       " ('take', 'VB'),\n",
       " ('read', 'NN'),\n",
       " ('around', 'IN'),\n",
       " ('probably', 'RB'),\n",
       " ('intp', 'JJ'),\n",
       " ('best', 'JJS'),\n",
       " ('thread', 'NN'),\n",
       " ('sometimes', 'RB'),\n",
       " ('years', 'NNS'),\n",
       " ('mean', 'VBP'),\n",
       " ('back', 'RB'),\n",
       " ('those', 'DT'),\n",
       " ('anything', 'NN'),\n",
       " ('maybe', 'RB'),\n",
       " ('little', 'JJ'),\n",
       " ('kind', 'NN'),\n",
       " ('usually', 'RB'),\n",
       " ('while', 'IN'),\n",
       " ('post', 'NN'),\n",
       " ('after', 'IN'),\n",
       " ('better', 'JJR'),\n",
       " ('look', 'NN'),\n",
       " ('might', 'MD'),\n",
       " ('long', 'RB'),\n",
       " ('different', 'JJ'),\n",
       " ('enfp', 'NN'),\n",
       " ('these', 'DT'),\n",
       " ('before', 'IN'),\n",
       " ('makes', 'VBZ'),\n",
       " ('seems', 'VBZ'),\n",
       " ('ever', 'RB'),\n",
       " ('understand', 'VB'),\n",
       " ('others', 'NNS'),\n",
       " ('world', 'NN'),\n",
       " ('everyone', 'NN'),\n",
       " ('since', 'IN'),\n",
       " ('trying', 'VBG'),\n",
       " ('thinking', 'VBG'),\n",
       " ('agree', 'JJ'),\n",
       " ('quite', 'RB'),\n",
       " ('through', 'IN'),\n",
       " ('hard', 'JJ'),\n",
       " ('both', 'DT'),\n",
       " ('point', 'NN'),\n",
       " ('made', 'VBD'),\n",
       " ('come', 'VBN'),\n",
       " ('tell', 'RB'),\n",
       " ('guess', 'JJ'),\n",
       " ('doing', 'VBG'),\n",
       " ('mind', 'NN'),\n",
       " ('types', 'NNS'),\n",
       " ('believe', 'VBP'),\n",
       " ('great', 'JJ'),\n",
       " ('talk', 'NN'),\n",
       " ('school', 'NN'),\n",
       " ('every', 'DT'),\n",
       " ('feeling', 'VBG'),\n",
       " ('down', 'RP'),\n",
       " ('entp', 'NNS'),\n",
       " ('used', 'VBN'),\n",
       " ('seem', 'VBP'),\n",
       " ('give', 'VB'),\n",
       " ('everything', 'NN'),\n",
       " ('having', 'VBG'),\n",
       " ('often', 'RB'),\n",
       " ('definitely', 'RB'),\n",
       " ('relationship', 'NN'),\n",
       " ('help', 'NN'),\n",
       " ('guys', 'VB'),\n",
       " ('last', 'JJ'),\n",
       " ('personality', 'NN'),\n",
       " ('times', 'NNS'),\n",
       " ('interesting', 'VBG'),\n",
       " ('anyone', 'NN'),\n",
       " ('question', 'NN'),\n",
       " ('another', 'DT'),\n",
       " ('enough', 'JJ'),\n",
       " ('part', 'NN'),\n",
       " ('idea', 'NN'),\n",
       " ('talking', 'VBG'),\n",
       " ('else', 'JJ'),\n",
       " ('hate', 'NN'),\n",
       " ('such', 'JJ'),\n",
       " ('tend', 'JJ'),\n",
       " ('getting', 'VBG'),\n",
       " ('least', 'JJS'),\n",
       " ('sense', 'JJ'),\n",
       " ('keep', 'VB'),\n",
       " ('mbti', 'NN'),\n",
       " ('once', 'RB'),\n",
       " ('problem', 'NN'),\n",
       " ('care', 'NN'),\n",
       " ('true', 'JJ'),\n",
       " ('either', 'RB'),\n",
       " ('without', 'IN'),\n",
       " ('year', 'NN'),\n",
       " ('found', 'VBD'),\n",
       " ('stuff', 'NN'),\n",
       " ('yourself', 'PRP'),\n",
       " ('nice', 'JJ'),\n",
       " ('thanks', 'NNS'),\n",
       " ('nothing', 'NN'),\n",
       " ('especially', 'RB'),\n",
       " ('music', 'NN'),\n",
       " ('between', 'IN'),\n",
       " ('reading', 'VBG'),\n",
       " ('thank', 'JJ'),\n",
       " ('experience', 'NN'),\n",
       " ('reason', 'NN'),\n",
       " ('wrong', 'JJ'),\n",
       " ('however', 'RB'),\n",
       " ('high', 'JJ'),\n",
       " ('again', 'RB'),\n",
       " ('sounds', 'VBZ'),\n",
       " ('start', 'JJ'),\n",
       " ('yeah', 'NN'),\n",
       " ('rather', 'RB'),\n",
       " ('test', 'RB'),\n",
       " ('live', 'JJ'),\n",
       " ('saying', 'VBG'),\n",
       " ('looking', 'VBG'),\n",
       " ('real', 'JJ'),\n",
       " ('enjoy', 'NN'),\n",
       " ('feelings', 'NNS'),\n",
       " ('almost', 'RB'),\n",
       " ('each', 'DT'),\n",
       " ('istj', 'JJ'),\n",
       " ('social', 'JJ'),\n",
       " ('done', 'VBN'),\n",
       " ('away', 'RB'),\n",
       " ('forum', 'JJ'),\n",
       " ('remember', 'VB'),\n",
       " ('happy', 'JJ'),\n",
       " ('comes', 'VBZ'),\n",
       " ('close', 'JJ'),\n",
       " ('sorry', 'JJ'),\n",
       " ('fact', 'NN'),\n",
       " ('already', 'RB'),\n",
       " ('relate', 'VB'),\n",
       " ('exactly', 'RB'),\n",
       " ('wanted', 'VBN'),\n",
       " ('answer', 'NN'),\n",
       " ('able', 'JJ'),\n",
       " ('istp', 'JJ'),\n",
       " ('girl', 'NN'),\n",
       " ('making', 'VBG'),\n",
       " ('enfj', 'JJ'),\n",
       " ('told', 'JJ'),\n",
       " ('whole', 'JJ'),\n",
       " ('place', 'NN'),\n",
       " ('until', 'IN'),\n",
       " ('functions', 'NNS'),\n",
       " ('change', 'VBP'),\n",
       " ('hope', 'VBP'),\n",
       " ('entj', 'RB'),\n",
       " ('less', 'JJR'),\n",
       " ('family', 'NN'),\n",
       " ('head', 'NN'),\n",
       " ('started', 'VBD'),\n",
       " ('interested', 'JJ'),\n",
       " ('alone', 'RB'),\n",
       " ('completely', 'RB'),\n",
       " ('isfp', 'JJ'),\n",
       " ('using', 'VBG'),\n",
       " ('seen', 'VBN'),\n",
       " ('show', 'NN'),\n",
       " ('isfj', 'JJ'),\n",
       " ('although', 'IN'),\n",
       " ('words', 'NNS'),\n",
       " ('wish', 'JJ'),\n",
       " ('past', 'IN'),\n",
       " ('welcome', 'JJ'),\n",
       " ('certain', 'JJ'),\n",
       " ('situation', 'NN'),\n",
       " ('sort', 'NN'),\n",
       " ('play', 'VB'),\n",
       " ('felt', 'JJ'),\n",
       " ('days', 'NNS'),\n",
       " ('along', 'IN'),\n",
       " ('general', 'JJ'),\n",
       " ('infps', 'NN'),\n",
       " ('today', 'NN'),\n",
       " ('must', 'MD'),\n",
       " ('thoughts', 'VB'),\n",
       " ('took', 'VBD'),\n",
       " ('emotional', 'JJ'),\n",
       " ('course', 'NN'),\n",
       " ('become', 'VBN'),\n",
       " ('mostly', 'RB'),\n",
       " ('example', 'NN'),\n",
       " ('stop', 'VB'),\n",
       " ('went', 'VBD'),\n",
       " ('similar', 'JJ'),\n",
       " ('ones', 'NNS'),\n",
       " ('likely', 'JJ'),\n",
       " ('emotions', 'NNS'),\n",
       " ('sound', 'VBP'),\n",
       " ('book', 'NN'),\n",
       " ('funny', 'JJ'),\n",
       " ('totally', 'RB'),\n",
       " ('means', 'VBZ'),\n",
       " ('hear', 'NN'),\n",
       " ('based', 'VBN'),\n",
       " ('frown', 'JJ'),\n",
       " ('pouting', 'VBG'),\n",
       " ('andry', 'JJ'),\n",
       " ('favorite', 'JJ'),\n",
       " ('personal', 'JJ'),\n",
       " ('weird', 'NN'),\n",
       " ('important', 'JJ'),\n",
       " ('dont', 'NN'),\n",
       " ('matter', 'NN'),\n",
       " ('strong', 'JJ')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the words pos\n",
    "nltk.pos_tag(most)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "digital-grain",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 535,
     "status": "ok",
     "timestamp": 1619650388004,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "static-customs",
    "outputId": "67298f6f-61f8-4e43-de6f-8bb00eba59d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words not Adj: 233\n",
      "Adjs: 67\n"
     ]
    }
   ],
   "source": [
    "#Checking the Words and ADJs\n",
    "notADJ = 0\n",
    "ADJ = 0\n",
    "for k,v in nltk.pos_tag(most):\n",
    "    if \"JJ\" not in v:\n",
    "        notADJ +=1\n",
    "    else:\n",
    "        ADJ +=1\n",
    "print('Words not Adj:', notADJ)\n",
    "print('Adjs:', ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "brief-injection",
   "metadata": {
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1619650412251,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "considered-advertising"
   },
   "outputs": [],
   "source": [
    "#We still have to remove the words from already in our dataframe column.\n",
    "wordsInDF = df.columns.values.tolist()\n",
    "#Column 10 is when the list of ADJ starts\n",
    "adjsInDF = wordsInDF[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "danish-authorization",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1619650415764,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "advisory-cemetery",
    "outputId": "13963c3b-1075-470b-950e-dab08fbccb03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of mostCommonList: 260\n",
      "New Length of word list with no Adj: 233\n"
     ]
    }
   ],
   "source": [
    "#Comparing the most list to our adjsInDF and removing the adjs from most common\n",
    "mostCommonList = [i for i in most if i not in adjsInDF]\n",
    "#Check by using len\n",
    "print(\"Length of mostCommonList:\",len(mostCommonList))\n",
    "#Note we still have ADJs in the list and it's not the full clean\n",
    "tagged = nltk.pos_tag(most)\n",
    "noAdjList = [x for x,y in tagged if 'JJ' not in y and x.isalpha()] #Cleaning out the JJ\n",
    "print(\"New Length of word list with no Adj:\", len(noAdjList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "distinguished-boring",
   "metadata": {
    "executionInfo": {
     "elapsed": 13387,
     "status": "ok",
     "timestamp": 1619650436354,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "coastal-vacuum"
   },
   "outputs": [],
   "source": [
    "#Applying the word list to each post using a function\n",
    "def mostCommon(post):\n",
    "    featureMost = {}\n",
    "    postList = list(set(post))\n",
    "    for i in noAdjList:\n",
    "        if i in postList:\n",
    "            featureMost[i] = 1\n",
    "        else:\n",
    "            featureMost[i] = 0\n",
    "    return(featureMost)\n",
    "\n",
    "df['MostCommon'] = df['Cleaner'].apply(lambda x: mostCommon(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "described-apparel",
   "metadata": {
    "executionInfo": {
     "elapsed": 24122,
     "status": "ok",
     "timestamp": 1619650464387,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "continental-investigator"
   },
   "outputs": [],
   "source": [
    "#Covert the keys in MostCommon it it's own Columns\n",
    "list_a = list(enumerate(df['MostCommon'][0].keys()))\n",
    "for index,lab in list_a:\n",
    "    df[lab] = df['MostCommon'].apply(lambda x: [v for k,v in x.items()][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "flush-venture",
   "metadata": {
    "executionInfo": {
     "elapsed": 1725,
     "status": "ok",
     "timestamp": 1619650669501,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "HaxOA2ITsKEn"
   },
   "outputs": [],
   "source": [
    "#Convert EmojiFeature in its own Column with one hot encoding\n",
    "list_b = list(enumerate(df['EmojiFeature'][0].keys()))\n",
    "for index,lab in list_b:\n",
    "    df[lab.strip('< >')] = df['EmojiFeature'].apply(lambda x: [v for k,v in x.items()][index])\n",
    "    df[lab.strip('< >')] = df[lab.strip('< >')].apply(lambda x: 1 if x==True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "peripheral-tulsa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P', 'S', 'N', 'T', 'I', 'J', 'F', 'E']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Breaking down myerTypes to each individuals types\n",
    "charList =list(set(''.join(list(set(df[\"myerTypes\"])))))\n",
    "charList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "improved-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that checks if the letters are in the type and assign it to 1 or 0\n",
    "def letter(myer, letter):\n",
    "    if letter in myer:\n",
    "        i = 1\n",
    "    else:\n",
    "        i = 0\n",
    "    return i\n",
    "for char in charList:\n",
    "    df[char]= df['myerTypes'].apply(lambda x: letter(x,char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dramatic-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Droping E,S,T,J to reduce redundancy\n",
    "df = df.drop(['E', 'S', 'T', 'J'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "welsh-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming I, N, F, P\n",
    "df.rename({'I': 'I(1)/E', 'P': 'P(1)/J','N': 'N(1)/S', 'F': 'F(1)/T'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "involved-racing",
   "metadata": {
    "executionInfo": {
     "elapsed": 587,
     "status": "ok",
     "timestamp": 1619650752752,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "growing-hampton"
   },
   "outputs": [],
   "source": [
    "#Removing the unused columns for the Classification\n",
    "df = df.drop(['EmojiFeature', 'ADJs', 'MostCommonWord', 'CombineDict', 'Clean', 'Cleaner', 'MostCommon'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "hybrid-soviet",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 735,
     "status": "ok",
     "timestamp": 1619651790966,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "demanding-violin",
    "outputId": "46ef273a-a071-4df2-d903-a973d1e2e9d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['myerTypes', 'AvgSentLength', 'AvgChar', 'Avg TFIDF', 'vaderScore',\n",
       "       'good', 'happy', 'much', 'cheeky', 'playful', 'big', 'sure',\n",
       "       'many', 'little', 'bad', 'different', 'best', 'great', 'new',\n",
       "       'sad', 'hard', 'last', 'right', 'first', 'old', 'high', 'infp',\n",
       "       'mean', 'infj', 'intj', 'welcome', 'intp', 'live', 'social',\n",
       "       'true', 'least', 'thread', 'nice', 'give', 'wrong', 'real',\n",
       "       'write', 'long', 'agree', 'able', 'strong', 'human', 'enfp',\n",
       "       'interested', 'entp', 'emotional', 'easy', 'young', 'certain',\n",
       "       'that', 'have', 'with', 'this', 'just', 'what', 'about', 'think',\n",
       "       'people', 'your', 'when', 'they', 'would', 'know', 'really',\n",
       "       'more', 'because', 'from', 'there', 'some', 'time', 'very', 'them',\n",
       "       'feel', 'being', 'been', 'love', 'things', 'want', 'something',\n",
       "       'will', 'most', 'only', 'also', 'then', 'could', 'than', 'well',\n",
       "       'does', 'always', 'even', 'type', 'here', 'someone', 'make',\n",
       "       'their', 'myself', 'never', 'life', 'find', 'into', 'thing',\n",
       "       'though', 'which', 'going', 'actually', 'person', 'pretty', 'were',\n",
       "       'friends', 'need', 'sticking', 'blowing', 'raspberry', 'still',\n",
       "       'work', 'thought', 'said', 'where', 'should', 'over', 'take',\n",
       "       'read', 'around', 'probably', 'sometimes', 'years', 'back',\n",
       "       'those', 'anything', 'maybe', 'kind', 'usually', 'while', 'post',\n",
       "       'after', 'look', 'might', 'these', 'before', 'makes', 'seems',\n",
       "       'ever', 'understand', 'others', 'world', 'everyone', 'since',\n",
       "       'trying', 'thinking', 'quite', 'through', 'both', 'point', 'made',\n",
       "       'come', 'tell', 'doing', 'mind', 'types', 'believe', 'talk',\n",
       "       'school', 'every', 'feeling', 'down', 'used', 'seem', 'everything',\n",
       "       'having', 'often', 'definitely', 'relationship', 'help', 'guys',\n",
       "       'personality', 'times', 'interesting', 'anyone', 'question',\n",
       "       'another', 'part', 'idea', 'talking', 'hate', 'getting', 'keep',\n",
       "       'mbti', 'once', 'problem', 'care', 'either', 'without', 'year',\n",
       "       'found', 'stuff', 'yourself', 'thanks', 'nothing', 'especially',\n",
       "       'music', 'between', 'reading', 'experience', 'reason', 'however',\n",
       "       'again', 'sounds', 'yeah', 'rather', 'test', 'saying', 'looking',\n",
       "       'enjoy', 'feelings', 'almost', 'each', 'done', 'away', 'remember',\n",
       "       'comes', 'fact', 'already', 'relate', 'exactly', 'wanted',\n",
       "       'answer', 'girl', 'making', 'place', 'until', 'functions',\n",
       "       'change', 'hope', 'entj', 'family', 'head', 'started', 'alone',\n",
       "       'completely', 'using', 'seen', 'show', 'although', 'words', 'past',\n",
       "       'situation', 'sort', 'play', 'days', 'along', 'infps', 'today',\n",
       "       'must', 'thoughts', 'took', 'course', 'become', 'mostly',\n",
       "       'example', 'stop', 'went', 'ones', 'emotions', 'sound', 'book',\n",
       "       'totally', 'means', 'hear', 'based', 'pouting', 'weird', 'dont',\n",
       "       'matter', 'laugh', 'frown sad andry pout', 'tongue', 'winky',\n",
       "       'wink', 'surprise', 'kitteh', 'unsure', 'dry', 'opsies',\n",
       "       'rolleyes', 'frustrate', 'mellow', 'wink smirk', 'sadness',\n",
       "       'angry', 'ninja', 'sad cry', 'hug', 'surprised', 'smile',\n",
       "       'frustrating', 'exterminate', 'laughing', 'happy face smiley',\n",
       "       'great dismay', 'perc', 'penguin', 'love_heart', 'kiss',\n",
       "       'hampster', 'joyous', 'th_love', 'lovekitty', 'skellie', 'confuse',\n",
       "       'ghost', 'P(1)/J', 'N(1)/S', 'I(1)/E', 'F(1)/T'], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the names of the columns\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "romance-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"ModelOneHots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "smooth-passenger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "charged-stress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>myerTypes</th>\n",
       "      <th>AvgSentLength</th>\n",
       "      <th>AvgChar</th>\n",
       "      <th>Avg TFIDF</th>\n",
       "      <th>vaderScore</th>\n",
       "      <th>good</th>\n",
       "      <th>happy</th>\n",
       "      <th>much</th>\n",
       "      <th>cheeky</th>\n",
       "      <th>playful</th>\n",
       "      <th>...</th>\n",
       "      <th>joyous</th>\n",
       "      <th>th_love</th>\n",
       "      <th>lovekitty</th>\n",
       "      <th>skellie</th>\n",
       "      <th>confuse</th>\n",
       "      <th>ghost</th>\n",
       "      <th>P(1)/J</th>\n",
       "      <th>N(1)/S</th>\n",
       "      <th>I(1)/E</th>\n",
       "      <th>F(1)/T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>13.6</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.042</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>11.8</td>\n",
       "      <td>5.52</td>\n",
       "      <td>0.023</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>14.3</td>\n",
       "      <td>5.84</td>\n",
       "      <td>0.032</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>11.9</td>\n",
       "      <td>5.93</td>\n",
       "      <td>0.029</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>11.5</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.032</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>14.6</td>\n",
       "      <td>5.81</td>\n",
       "      <td>0.032</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.80</td>\n",
       "      <td>0.019</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>INTP</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5.99</td>\n",
       "      <td>0.033</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>INFP</td>\n",
       "      <td>20.5</td>\n",
       "      <td>5.65</td>\n",
       "      <td>0.022</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>INFP</td>\n",
       "      <td>18.4</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.025</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows × 357 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     myerTypes  AvgSentLength  AvgChar  Avg TFIDF  vaderScore  good  happy  \\\n",
       "0         INFJ           13.6     5.75      0.042           2     1      0   \n",
       "1         ENTP           11.8     5.52      0.023           2     1      1   \n",
       "2         INTP           14.3     5.84      0.032           2     1      1   \n",
       "3         INTJ           11.9     5.93      0.029           2     0      0   \n",
       "4         ENTJ           11.5     5.88      0.032           2     1      0   \n",
       "...        ...            ...      ...        ...         ...   ...    ...   \n",
       "8670      ISFP           14.6     5.81      0.032           2     1      1   \n",
       "8671      ENFP           14.0     5.80      0.019           2     1      1   \n",
       "8672      INTP            9.6     5.99      0.033           2     1      0   \n",
       "8673      INFP           20.5     5.65      0.022           2     1      0   \n",
       "8674      INFP           18.4     5.50      0.025           2     1      1   \n",
       "\n",
       "      much  cheeky  playful  ...  joyous  th_love  lovekitty  skellie  \\\n",
       "0        1       1        1  ...       0        0          0        0   \n",
       "1        1       1        0  ...       0        0          0        0   \n",
       "2        1       0        0  ...       0        0          0        0   \n",
       "3        1       0        1  ...       0        0          0        0   \n",
       "4        1       0        0  ...       0        0          0        0   \n",
       "...    ...     ...      ...  ...     ...      ...        ...      ...   \n",
       "8670     1       0        0  ...       0        0          0        0   \n",
       "8671     1       1        1  ...       0        0          0        0   \n",
       "8672     0       1        1  ...       0        0          0        0   \n",
       "8673     1       0        0  ...       0        0          0        0   \n",
       "8674     1       1        1  ...       0        0          0        0   \n",
       "\n",
       "      confuse  ghost  P(1)/J  N(1)/S  I(1)/E  F(1)/T  \n",
       "0           0      0       0       1       1       1  \n",
       "1           0      0       1       1       0       0  \n",
       "2           0      0       1       1       1       0  \n",
       "3           0      0       0       1       1       0  \n",
       "4           0      0       0       1       0       0  \n",
       "...       ...    ...     ...     ...     ...     ...  \n",
       "8670        0      0       1       0       1       1  \n",
       "8671        0      0       1       1       0       1  \n",
       "8672        0      0       1       1       1       0  \n",
       "8673        0      0       1       1       1       1  \n",
       "8674        0      0       1       1       1       1  \n",
       "\n",
       "[8675 rows x 357 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-bernard",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ModelAccuracy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
